"""
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è:
- –ü–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
"""

import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
from uuid import UUID

from celery import group, chain, chord
from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession

from src.config.settings import get_settings
from src.database.connection import get_async_session
from src.database.models import RawSignal, Feature, Equipment, Prediction, ProcessingStatus
from src.data_processing.csv_loader import CSVLoader
from src.utils.logger import get_logger
from src.worker.tasks import celery_app, DatabaseTask, process_raw, detect_anomalies, forecast_trend

settings = get_settings()
logger = get_logger(__name__)


@celery_app.task(bind=True, base=DatabaseTask)
def batch_process_directory(self, directory_path: str, equipment_id: Optional[str] = None) -> Dict:
    """
    –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö CSV —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏

    Args:
        directory_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å CSV —Ñ–∞–π–ª–∞–º–∏
        equipment_id: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π ID –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    self.logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞–∫–µ—Ç–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {directory_path}")

    try:
        result = asyncio.run(_batch_process_directory_async(directory_path, equipment_id))
        self.logger.info(f"–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {result['total_files']} —Ñ–∞–π–ª–æ–≤")
        return result

    except Exception as exc:
        self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: {exc}")
        raise exc


async def _batch_process_directory_async(directory_path: str, equipment_id: Optional[str]) -> Dict:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""

    directory = Path(directory_path)
    if not directory.exists():
        raise ValueError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory_path}")

    csv_files = list(directory.glob("*.csv"))
    if not csv_files:
        raise ValueError(f"CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {directory_path}")

    loader = CSVLoader()
    processed_files = []
    failed_files = []

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º equipment_id –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
    if not equipment_id:
        equipment_id = await _get_or_create_equipment_from_filename(csv_files[0].name)

    for csv_file in csv_files:
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV —Ñ–∞–π–ª
            stats = await loader.load_csv_file(
                file_path=str(csv_file),
                equipment_id=UUID(equipment_id)
            )

            processed_files.append({
                'file': str(csv_file),
                'stats': stats.__dict__,
                'status': 'success'
            })

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞–∂–¥–æ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
            if hasattr(stats, 'raw_signal_ids'):
                for raw_id in stats.raw_signal_ids:
                    process_raw.delay(str(raw_id))

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {csv_file}: {e}")
            failed_files.append({
                'file': str(csv_file),
                'error': str(e),
                'status': 'failed'
            })

    return {
        'status': 'success',
        'directory': directory_path,
        'equipment_id': equipment_id,
        'total_files': len(csv_files),
        'processed_files': len(processed_files),
        'failed_files': len(failed_files),
        'details': {
            'processed': processed_files,
            'failed': failed_files
        }
    }


async def _get_or_create_equipment_from_filename(filename: str) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""

    # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ID –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    # –ù–∞–ø—Ä–∏–º–µ—Ä: motor_001.csv -> EQ_2025_000001
    base_name = Path(filename).stem
    equipment_name = base_name.replace('_', ' ').title()

    async with get_async_session() as session:
        # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ
        query = select(Equipment).where(Equipment.name.ilike(f"%{equipment_name}%"))
        result = await session.execute(query)
        equipment = result.scalar_one_or_none()

        if equipment:
            return str(equipment.id)

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ
        new_equipment = Equipment(
            name=equipment_name,
            equipment_type='motor',
            model=f"Auto-detected from {filename}",
            location='Unknown',
            is_active=True,
            created_at=datetime.utcnow()
        )

        session.add(new_equipment)
        await session.commit()
        await session.refresh(new_equipment)

        logger.info(f"–°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ: {equipment_name} ({new_equipment.id})")

        return str(new_equipment.id)


@celery_app.task(bind=True, base=DatabaseTask)
def process_equipment_workflow(self, equipment_id: str, force_reprocess: bool = False) -> Dict:
    """
    –ü–æ–ª–Ω—ã–π —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è

    Args:
        equipment_id: ID –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
        force_reprocess: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    self.logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è {equipment_id}")

    try:
        result = asyncio.run(_process_equipment_workflow_async(equipment_id, force_reprocess))
        self.logger.info(f"–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –¥–ª—è {equipment_id} –∑–∞–≤–µ—Ä—à–µ–Ω")
        return result

    except Exception as exc:
        self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –¥–ª—è {equipment_id}: {exc}")
        raise exc


async def _process_equipment_workflow_async(equipment_id: str, force_reprocess: bool) -> Dict:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è"""

    async with get_async_session() as session:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—ã—Ä—ã–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
        status_filter = [ProcessingStatus.PENDING]
        if force_reprocess:
            status_filter.extend([ProcessingStatus.COMPLETED, ProcessingStatus.FAILED])

        query = select(RawSignal).where(
            RawSignal.equipment_id == UUID(equipment_id),
            RawSignal.processing_status.in_(status_filter)
        )

        result = await session.execute(query)
        raw_signals = result.scalars().all()

        if not raw_signals:
            return {
                'status': 'no_data',
                'equipment_id': equipment_id,
                'message': '–ù–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏'
            }

        # –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫—É –∑–∞–¥–∞—á –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
        workflow_tasks = []

        for raw_signal in raw_signals:
            # –¶–µ–ø–æ—á–∫–∞: –æ–±—Ä–∞–±–æ—Ç–∫–∞ -> –¥–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
            signal_workflow = chain(
                process_raw.s(str(raw_signal.id)),
                # –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—É—Å–∫–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é –¥–ª—è –≤—Å–µ—Ö –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                _create_anomaly_detection_chord.s()
            )
            workflow_tasks.append(signal_workflow)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ü–µ–ø–æ—á–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        job = group(workflow_tasks)
        workflow_result = job.apply_async()

        # –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–æ–∫ –∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
        forecast_task = forecast_trend.apply_async(
            args=[equipment_id],
            countdown=300  # –ó–∞–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ 5 –º–∏–Ω—É—Ç –ø–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        )

        return {
            'status': 'started',
            'equipment_id': equipment_id,
            'signals_to_process': len(raw_signals),
            'workflow_job_id': workflow_result.id,
            'forecast_task_id': forecast_task.id,
            'force_reprocess': force_reprocess
        }


@celery_app.task
def _create_anomaly_detection_chord(process_result: Dict) -> str:
    """–°–æ–∑–¥–∞–Ω–∏–µ chord –∑–∞–¥–∞—á–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π"""

    if process_result.get('status') != 'success':
        return f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–≥–Ω–∞–ª–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {process_result}"

    feature_ids = process_result.get('feature_ids', [])
    if not feature_ids:
        return "–ù–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏"

    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    detection_tasks = [detect_anomalies.s(feature_id) for feature_id in feature_ids]

    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    detection_job = group(detection_tasks)
    result = detection_job.apply_async()

    return f"–ó–∞–ø—É—â–µ–Ω–∞ –¥–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è {len(feature_ids)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {result.id}"


@celery_app.task(bind=True, base=DatabaseTask)
def health_check_system(self) -> Dict:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

    Returns:
        –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    """
    self.logger.info("–í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã")

    try:
        result = asyncio.run(_health_check_system_async())
        return result

    except Exception as exc:
        self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã: {exc}")
        raise exc


async def _health_check_system_async() -> Dict:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""

    async with get_async_session() as session:
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—ã—Ä—ã–º —Å–∏–≥–Ω–∞–ª–∞–º
        raw_stats = await session.execute(
            select(
                RawSignal.processing_status,
                func.count(RawSignal.id).label('count')
            ).group_by(RawSignal.processing_status)
        )

        raw_signal_stats = {row.processing_status.value: row.count for row in raw_stats}

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        features_count = await session.execute(
            select(func.count(Feature.id))
        )
        total_features = features_count.scalar()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ–≥–Ω–æ–∑–∞–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
        yesterday = datetime.utcnow() - timedelta(days=1)
        recent_predictions = await session.execute(
            select(func.count(Prediction.id)).where(
                Prediction.created_at >= yesterday
            )
        )
        recent_predictions_count = recent_predictions.scalar()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∞–Ω–æ–º–∞–ª–∏—è–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
        recent_anomalies = await session.execute(
            select(func.count(Prediction.id)).where(
                Prediction.created_at >= yesterday,
                Prediction.anomaly_detected == True
            )
        )
        recent_anomalies_count = recent_anomalies.scalar()

        # –ê–∫—Ç–∏–≤–Ω–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ
        active_equipment = await session.execute(
            select(func.count(Equipment.id)).where(
                Equipment.is_active == True
            )
        )
        active_equipment_count = active_equipment.scalar()

        # –û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ —Å –Ω–µ–¥–∞–≤–Ω–∏–º–∏ –∞–Ω–æ–º–∞–ª–∏—è–º–∏
        equipment_with_anomalies = await session.execute(
            select(func.count(func.distinct(Prediction.equipment_id))).where(
                Prediction.created_at >= yesterday,
                Prediction.anomaly_detected == True
            )
        )
        equipment_with_anomalies_count = equipment_with_anomalies.scalar()

        return {
            'status': 'healthy',
            'timestamp': datetime.utcnow().isoformat(),
            'statistics': {
                'raw_signals': {
                    'by_status': raw_signal_stats,
                    'total': sum(raw_signal_stats.values())
                },
                'features': {
                    'total': total_features
                },
                'predictions_24h': {
                    'total': recent_predictions_count,
                    'anomalies': recent_anomalies_count,
                    'anomaly_rate': recent_anomalies_count / max(1, recent_predictions_count)
                },
                'equipment': {
                    'active': active_equipment_count,
                    'with_recent_anomalies': equipment_with_anomalies_count,
                    'anomaly_equipment_rate': equipment_with_anomalies_count / max(1, active_equipment_count)
                }
            },
            'alerts': _generate_system_alerts(
                raw_signal_stats,
                recent_anomalies_count,
                equipment_with_anomalies_count,
                active_equipment_count
            )
        }


def _generate_system_alerts(
    raw_signal_stats: Dict,
    recent_anomalies: int,
    equipment_with_anomalies: int,
    active_equipment: int
) -> List[Dict]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤"""

    alerts = []

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –æ–±—Ä–∞–±–æ—Ç–æ–∫
    failed_signals = raw_signal_stats.get('failed', 0)
    total_signals = sum(raw_signal_stats.values())

    if total_signals > 0 and failed_signals / total_signals > 0.1:  # >10% –Ω–µ—É–¥–∞—á
        alerts.append({
            'level': 'warning',
            'type': 'processing_failures',
            'message': f'–í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –Ω–µ—É–¥–∞—á–Ω—ã—Ö –æ–±—Ä–∞–±–æ—Ç–æ–∫: {failed_signals}/{total_signals} ({failed_signals/total_signals*100:.1f}%)'
        })

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∞–Ω–æ–º–∞–ª–∏–π
    if recent_anomalies > 50:  # –ú–Ω–æ–≥–æ –∞–Ω–æ–º–∞–ª–∏–π –∑–∞ –¥–µ–Ω—å
        alerts.append({
            'level': 'warning',
            'type': 'high_anomaly_count',
            'message': f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–Ω–æ–≥–æ –∞–Ω–æ–º–∞–ª–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞: {recent_anomalies}'
        })

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–ª–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è —Å –∞–Ω–æ–º–∞–ª–∏—è–º–∏
    if active_equipment > 0 and equipment_with_anomalies / active_equipment > 0.3:  # >30% –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
        alerts.append({
            'level': 'critical',
            'type': 'widespread_anomalies',
            'message': f'–ê–Ω–æ–º–∞–ª–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è: {equipment_with_anomalies}/{active_equipment} ({equipment_with_anomalies/active_equipment*100:.1f}%)'
        })

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞—Å—Ç—Ä—è–≤—à–∏—Ö –∑–∞–¥–∞—á
    pending_signals = raw_signal_stats.get('processing', 0)
    if pending_signals > 100:
        alerts.append({
            'level': 'warning',
            'type': 'processing_backlog',
            'message': f'–ë–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –æ—á–µ—Ä–µ–¥–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {pending_signals}'
        })

    return alerts


@celery_app.task(bind=True, base=DatabaseTask)
def daily_equipment_report(self, equipment_id: Optional[str] = None) -> Dict:
    """
    –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ —Å–æ—Å—Ç–æ—è–Ω–∏—é –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è

    Args:
        equipment_id: ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –∏–ª–∏ None –¥–ª—è –≤—Å–µ–≥–æ

    Returns:
        –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á–µ—Ç
    """
    self.logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è: {equipment_id or '–≤—Å—ë'}")

    try:
        result = asyncio.run(_daily_equipment_report_async(equipment_id))
        return result

    except Exception as exc:
        self.logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {exc}")
        raise exc


async def _daily_equipment_report_async(equipment_id: Optional[str]) -> Dict:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""

    async with get_async_session() as session:
        yesterday = datetime.utcnow() - timedelta(days=1)

        # –§–∏–ª—å—Ç—Ä –ø–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—é
        equipment_filter = []
        if equipment_id:
            equipment_filter.append(Equipment.id == UUID(equipment_id))

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—é
        equipment_query = select(Equipment).where(*equipment_filter, Equipment.is_active == True)
        equipment_result = await session.execute(equipment_query)
        equipment_list = equipment_result.scalars().all()

        equipment_reports = []

        for equipment in equipment_list:
            # –ê–Ω–æ–º–∞–ª–∏–∏ –∑–∞ –¥–µ–Ω—å
            anomalies_query = select(func.count(Prediction.id)).where(
                Prediction.equipment_id == equipment.id,
                Prediction.created_at >= yesterday,
                Prediction.anomaly_detected == True
            )
            anomalies_count = (await session.execute(anomalies_query)).scalar()

            # –ü–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–≥–Ω–æ–∑
            last_forecast_query = select(Prediction).where(
                Prediction.equipment_id == equipment.id,
                Prediction.model_name == 'rms_trend_forecasting'
            ).order_by(Prediction.created_at.desc()).limit(1)

            last_forecast_result = await session.execute(last_forecast_query)
            last_forecast = last_forecast_result.scalar_one_or_none()

            # –°—Ç–∞—Ç—É—Å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
            status = 'normal'
            if anomalies_count > 10:
                status = 'critical'
            elif anomalies_count > 5:
                status = 'warning'
            elif anomalies_count > 0:
                status = 'attention'

            equipment_reports.append({
                'equipment_id': str(equipment.id),
                'equipment_name': equipment.name,
                'status': status,
                'anomalies_24h': anomalies_count,
                'last_forecast': {
                    'timestamp': last_forecast.created_at.isoformat() if last_forecast else None,
                    'max_anomaly_probability': last_forecast.confidence if last_forecast else None,
                    'recommendation': last_forecast.prediction_data.get('forecast_summary', {}).get('recommendation') if last_forecast and last_forecast.prediction_data else None
                }
            })

        # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
        total_equipment = len(equipment_reports)
        critical_equipment = len([r for r in equipment_reports if r['status'] == 'critical'])
        warning_equipment = len([r for r in equipment_reports if r['status'] == 'warning'])
        total_anomalies = sum(r['anomalies_24h'] for r in equipment_reports)

        return {
            'report_date': yesterday.date().isoformat(),
            'generated_at': datetime.utcnow().isoformat(),
            'summary': {
                'total_equipment': total_equipment,
                'critical_equipment': critical_equipment,
                'warning_equipment': warning_equipment,
                'normal_equipment': total_equipment - critical_equipment - warning_equipment,
                'total_anomalies_24h': total_anomalies
            },
            'equipment_details': equipment_reports,
            'recommendations': _generate_daily_recommendations(equipment_reports)
        }


def _generate_daily_recommendations(equipment_reports: List[Dict]) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""

    recommendations = []

    critical_equipment = [r for r in equipment_reports if r['status'] == 'critical']
    warning_equipment = [r for r in equipment_reports if r['status'] == 'warning']

    if critical_equipment:
        recommendations.append(
            f"üî¥ –ö–†–ò–¢–ò–ß–ù–û: –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ: {', '.join([e['equipment_name'] for e in critical_equipment])}"
        )

    if warning_equipment:
        recommendations.append(
            f"üü° –í–ù–ò–ú–ê–ù–ò–ï: –ó–∞–ø–ª–∞–Ω–∏—Ä—É–π—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è: {', '.join([e['equipment_name'] for e in warning_equipment])}"
        )

    high_anomaly_equipment = [r for r in equipment_reports if r['anomalies_24h'] > 20]
    if high_anomaly_equipment:
        recommendations.append(
            f"üìä –ü–æ–≤—ã—à–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–π: {', '.join([e['equipment_name'] for e in high_anomaly_equipment])}"
        )

    if not critical_equipment and not warning_equipment:
        recommendations.append("‚úÖ –í—Å–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ")

    return recommendations


# –≠–∫—Å–ø–æ—Ä—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
__all__ = [
    'batch_process_directory',
    'process_equipment_workflow',
    'health_check_system',
    'daily_equipment_report'
]
